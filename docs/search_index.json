[["index.html", "Grad’s Declassified Data Survival Guide Chapter 1 Introduction 1.1 What is this guide? 1.2 How to use this resource", " Grad’s Declassified Data Survival Guide Summer Mengarelli and Mikala Narlock Chapter 1 Introduction 1.1 What is this guide? This guide is designed for graduate students working in a research lab who are tasked with managing research data. In this guide, you will find individual lessons on specific aspects of data management. These are geared towards active data management, meaning projects that are in process. These data are being updated, collected, aggregated, processed, cleaned, or analyzed on a regular basis. In other words, the focus of this material is how to integrate solid data management practices into your work without requiring a complete retooling of existing workflows and processes. This guide has been published through bookdown via GitHub. [Publication date of the guide, citation, w/e] This guide has been written by two librarians who are also active researchers. Each author has joined existing projects and launched new ones. We have pulled on best practices from our education and lived experiences. 1.2 How to use this resource While the content of each lesson varies, you will generally learn definitions, recommended practices, and tips for integrating said practices into new and existing workflows. Each lesson is designed to be a standalone object for point of need adoption. In other words, when you realize you need more information about annotating code, creating a README, or keeping your data safe, you can jump right to that lesson. The lessons are ordered roughly according to a typical research lifecycle, but it is not necessary to complete prior lessons in order to understand and apply what you need. This guide should be considered a starting point: the general recommendations may not be specific enough for your needs. In each lesson, we have provided additional resources and links to other portions of the guide. However, there are many resources available online and likely at your institution to provide the support you may need – we suggest looking into whether your institution’s library has support for research data management, or reaching out to your institutional office of research. All resources are only as valuable as they are to you. While we have captured and distilled recommended practices in this guide, consider which of these are useful or not in your research. "],["situating-yourself-in-the-research-lifecycle.html", "Chapter 2 Situating Yourself in the Research Lifecycle 2.1 Learning Objective 2.2 Key Terms 2.3 Lesson 2.4 Further Reading", " Chapter 2 Situating Yourself in the Research Lifecycle Or, how to navigate through data management. 2.1 Learning Objective Understand the general research lifecycle and how robust data management practices can mitigate challenges caused by temporal gaps in research projects. 2.2 Key Terms Research Lifecycle: The process of conducting research, from inception, through data collection and analysis, to publication. Presented as a lifecycle, as data and publications can be reused by other researchers in the design of additional projects. Research Data Management: The processes, tools, and techniques necessary for collecting, analysing, storing, and publishing data. Temporal Gaps: Periods of time between research activities. 2.3 Lesson All research projects follow a general lifecycle: from planning and design, through to implementation (which includes data collection and analysis), and concluding with concluding with output sharing (including research publishing and data depositing) and reuse, where the cycle begins again, with a new iteration of the same project or the development and launch of a new investigation. \\[add in lifecycle image from Harvard\\] Cioffi, M., Goldman, J., &amp; Marchese, S. (2023). Harvard Biomedical Research Data Lifecycle (Version 5). Zenodo. https://doi.org/10.5281/zenodo.8076168 While the lifecycle image invokes a tidy overview of how research projects generally progress, the reality is often more complex. For example, a project may require iterations of design and data collection before moving to analysis and publication. This can be compounded by changes in disciplinary research practices and policies at institutional, state, and federal levels. This messy reality of research projects means that there can be temporal gaps in research; in other words, for various reasons, projects may start and stop over time. These temporal gaps can be most acutely felt as personnel join and depart projects. Robust data management practices can be invaluable for closing those gaps and easing team members’ transitions in and out of a research project. In the remainder of this guide, you’ll learn more about different aspects of research data management that will make it easier for you to get acclimated to your lab’s research data, as well as prepare it for your eventual departure from the lab/project (which in turn will make it easier for the next research collaborator!). \\[add in a mechanism to encourage students to click the next one, like a table of contents or sentence based links to the rest of the content\\] 2.4 Further Reading Research lifecycle guide (Princeton) Research Data Management Workbook Data Management and Sharing Activities (ARL) "],["getting-oriented-to-a-dataset.html", "Chapter 3 Getting Oriented to a Dataset 3.1 Learning Objective 3.2 Key Terms 3.3 Lesson 3.4 Conclusion 3.5 Exercises 3.6 Further Readings 3.7 References", " Chapter 3 Getting Oriented to a Dataset Or, how to find your bearings in an existing dataset. 3.1 Learning Objective Learn and adapt data curation strategies to become acclimated to a dataset you’ve inherited. 3.2 Key Terms Data Curation: The tasks by which data is reviewed to ensure it is fit for purpose. Directory: A folder within a computer’s file system, which might contain other folders (subdirectories) and/or files. README: A document, typically written in Markdown or as plain text, that serves as the front page or abstract of a project’s materials – for example, the source code and data files used in a research project – and conveys information about the purpose, creation, organization, and use of the materials. Data Dictionary: Documentation that accompanies a dataset, listing and defining each variable and providing information on data types, conventions followed, and valid/possible values for each variable. Raw data: Here, data in the state it is in when you generate/access/receive it. ORCID iD: A unique persistent identifier to disambiguate researchers and track research outputs. 3.3 Lesson When starting or joining an existing project, or even taking previously created data to launch a new project, it is crucial to spend time acclimating to the dataset you plan to use. Spending the time now to get oriented will help avoid errors. However, approaching a dataset without a plan, especially if it is a complicated or particularly large resource, can be overwhelming. To help with that, integrate data curation strategies. Data curation, or the tasks by which data is reviewed to ensure it is fit for purpose, provides a structured method to engage with data. In general, data curation tasks include: Creating a comprehensive listing of files in the dataset. If a listing already exists, making sure all files are present. Reviewing the documentation to ensure variables are defined, methodology is present and clear, and descriptive information is thorough. If applicable, running code to ensure it works as expected, dependencies are named, and code has clear annotation (link to Annotating Code). This process often occurs at the end of a research project, when outputs are being published in repositories. However, these practices can be adopted and adapted for finding your way through a new and unfamiliar dataset, since data curation strategies offer standardized yet flexible approaches to understanding a dataset. Below, we offer a general framework, divided into practices.1 This process can be used for all data types, across all disciplines. 3.3.1 Practice #1: Check and Understand the documentation and dataset First and foremost, get access to the data and documentation. This may seem like a no brainer, but sometimes data can be distributed in multiple locations. Make sure you have access to all of the data and affiliated documentation. Ask your PI, peers, and others in the lab about data and affiliated documentation to make sure you have access to everything. This may require getting access to servers, added to shared drives or networked storage, or even being shown where the physical records are kept. You may even need to follow hyperlinks that lead to other storage folders– that is ok! This process may take time. Do not rush this step though, as all future steps build from it. After that, begin by reviewing the documentation. Look for READMEs, dictionaries, or other overview documentation that will help you learn more about the methodology, expected data files, and the project overall. If this is an ongoing project which has been written about before, and you have not already, consider reading any previous publications or reviewing any publicly available datasets. ACTION: As you review the documentation, start your own review log. This is for your use, and your use only– so don’t be afraid to add to it, remove from it, whatever you need to do to get your thoughts together. Use it as a way to annotate your journey through the documentation, which you’ll expand on. ———————————————- TEMPLATE SNIPPET: Documentation reviewed Dictionary Important points Questions README Important points Questions Article(s) Important points Questions … ———————————————- Now, repeat the process with the actual data files. As much as possible, avoid making copies of the data, as that will lead to file proliferation. In general, you want to create as few derivatives of key files as reasonably possible to avoid confusion as to which file is the correct version– especially when you are collaborating in a lab. That being said, you will need to take care reviewing the raw data files, or the source data collected from instruments or observation from which other data or visualizations will be derived, when possible. Especially as you are getting your bearings in the data, it can be easy to accidentally alter or even delete data, occasionally in an irreversible way. If you feel you absolutely must make copies of the data, store the files in a working directory on your local machine (if possible given the size of the data) and remove the files as soon as you have completed your review. ACTION: Here, document what you’ve reviewed and questions you may have. Pay close attention to the folders that you have reviewed so you do not accidentally revisit data– but don’t be afraid to circle back to documentation or other parts of the data as you are forming connections to the different materials! ———————————————- TEMPLATE SNIPPET: Data reviewed \\[Directory 1\\] Important points Questions \\[Directory 2\\] Important points Questions \\[Directory 3\\] Important points Questions … ———————————————- 3.3.2 Practice 2: Request information and augment the documentation After you have explored the data and documentation, pause to synthesize for yourself: what have you learned? What questions do you still have? ACTION: At this point, divide your questions into three categories: Critical, Important, and Nice to know. Critical: These questions must be answered before you feel comfortable proceeding with your role. While this will depend on your position and research, some examples of critical questions include: Undefined or unclear variables Missing units of measurement Undefined or unclear relationships between different files Important: These questions need to be answered soon, but are not an impediment to your role. Some examples of important questions include: Data versioning process and how to document changes to the data Unclear data analysis processes, such as via scripts Quality assurance or review process for data collection Data storage and backup mechanisms Nice to know: These questions will provide useful information at a future stage of the research project. Consider asking them at regular meetings with your PI or peers. Some examples of nice to know questions include: Who has access to the data currently The long-term plan for sharing or destroying the data At this stage, also consider who needs to answer each question. All of this is leading to an important rule of thumb: in order to get the answers you need, ask no more than 4 questions at once– especially via email– of one person (Hudson Vitale et al, 2024). This will help ensure you get the answers you need to be successful. Once you have the critical questions answered, consider if there are changes you can make to the documentation in isolation. For example, if you have learned what a snippet of code does, which is currently not annotated, can you add that in? Can you draft a README? In contrast, you likely cannot change how data are being collected or analyzed. Note: There may be no changes you are comfortable making on your own– that is ok! You can also ask a colleague or your PI to review the proposed changes before you make them. ACTION: Using what you’ve learned, augment the dataset documentation. If you are not comfortable updating the project-level documentation, at the minimum update your personal documentation. 3.3.3 Practice 3: Transform practices and evaluate regularly As you transition from learning about the dataset to contributing to it, remain cognizant of the workflows and documentation. Consider, in particular, the questions you initially had when looking through the data. Are there ways you could consistently improve the practices of the lab to make it easier for future students? Revisit this question on a periodic basis, such as quarterly. ACTION: Over time, especially as you feel more secure in the project or lab, consider suggesting changes to the data management process. This could include things like, adjusting the file naming convention, implementing a more robust version management tool, or even just improving the documentation. This may not be something you can do right away– but it is never too late to integrate small changes to have a big impact on the research data. ACTION: Add a quarterly meeting to your calendar, or even your project team’s calendar, to review your processes and consider what can be improved. ———————————————- TEMPLATE SNIPPET: Data Management Practices Quarterly Review Meeting (Team wide) Did anyone leave the lab that needs to be removed from the project (in other words, are there any accounts that need to be removed to ensure no one outside of the project has access) Are there any files that need to be deleted Has our documentation been updated recently Can we confirm our backup system is running as expected Do our files all follow the file-naming convention During the last three months, have we made any changes to our collection process? If yes, have they been documented? During the last three months, have there been any challenges to the data collection or management process? If yes, can we mitigate these by changing our workflow? Data Management Practices Quarterly Review (self) Do I have any duplicate or extraneous files on my local machine that need to be deleted Is my local documentation up to date? Has this/can it be added to the team documentation? Is my code documented ———————————————- 3.3.4 Practice 4: Document your work During your conversations, when you make data management decisions, or change processes, be sure to capture everything. Even in situations where no changes are made, document why and if the topic should be revisited at a future date. Combining the snippets from above, and repeated as necessary, you have your Data Management Log! Some good practices for your documentation: Create a header for your documentation Created by: (Name, ORCiD) Role in lab/project: Date range covered: (Start Date-End date) Add dates to each entry Add entries in reverse chronological order (more recent info goes at the top) Be specific. Do not assume you will remember an abbreviation in six months. ACTION: Review your documentation alongside your data management practices, approximately quarterly. Documentation can be easy to fall behind on, but staying on top of it now will make future efforts (such as writing the methodology of you paper) easier. ———————————————- DATA MANAGEMENT LOG TEMPLATE Created by: (Name, ORCiD) Role in lab/project: Date range covered: (Start Date-End date) YYYY-MM-DD Changes made to data files (generally) Documentation updated Code created, edit …. ———————————————- 3.4 Conclusion As you progress in your career, remember: You are the first curator of your data. All actions you take shape the final outcome of a dataset and how reusable (or not) it is, both to yourself and future researchers. 3.5 Exercises Using the practice dataset… 3.6 Further Readings Data Curation Primers: Created and managed by the Data Curation Network, these are resources created for data stewards when reviewing different data types. CURATE(D) Steps: The process by which data curators review datasets. Generalized workflow, mostly for teaching. 3.7 References Hudson Vitale, Cynthia; Hadley, Hannah; Wham, Briana; Borda, Susan; Carlson, Jake; Darragh, Jennifer; Fearon, David; Herndon, Joel; Hunt, Shanda; Johnston, Lisa R.; Kalt, Marley; Kozlowski, Wendy; Lafferty-Hess, Sophia; Marsolek, Wanda; Moore, Jennifer; Narlock, Mikala; Scott, Dorris; Wheeler, Jon; Woodbrook, Rachel; Wright, Sarah; Yee, Michelle; Lake, Sherry. (2024). CURATE(D) Fundamentals Workshop. Retrieved from the University Digital Conservancy, https://doi.org/10.24926/2024.265916. These have been adapted from the Data Curation Network’s CURATE(D) Steps.↩︎ "],["navigating-and-organizing-files.html", "Chapter 4 Navigating and Organizing Files 4.1 Lesson Objective 4.2 Key Terms 4.3 Lesson 4.4 Conclusion 4.5 Exercises 4.6 Further Readings", " Chapter 4 Navigating and Organizing Files Or, how to keep your project materials neatly stored. 4.1 Lesson Objective Understand the directory structure on your computer, file naming conventions and how to use them to organize your project files. 4.2 Key Terms Directory: A folder within a computer’s file system, which might contain other folders (subdirectories) and/or files. Hierarchical Structure: A structure defined by objects nested within objects, stemming from a single, highest-level “root” object. In the case of a computer’s file system, this refers to the way that files and subdirectories are nested within higher-level directories, up to the root of the system. Absolute Path: The path, or address, from the root of a computer’s file system to a directory or file of interest. Raw Data: Here, data in the state it is in when you generate/access/receive it. Relative Path: The path, or address, to a directory or file of interest, relative to the current working directory. Working Directory: The directory within your computer’s file system that you are currently “in;” the directory that a computer program can currently see or access. 4.3 Lesson Taking a few minutes to organize your research project files is one of the easiest steps you can take to help yourself and potential collaborators in the future. It may be tempting to let files rest where they lie – a desktop or a Google Drive, for example – and simply use your computer or Google’s search bar to find files by their names. Maybe you’ve already taken the extra step of establishing a folder specifically for this project, and you make sure that you save all new or altered project files in that folder. This is a good first step, but a few more intentional layers of organization will be hugely beneficial to you, both now and in the future. This lesson will introduce you to your computer’s file system (which you’ve almost certainly traversed, but may not fully understand) and give some brief suggestions for how to organize your project within that system. It also covers file naming conventions – why they’re worth thinking about, and how you might implement them. 4.3.1 The hierarchy of a computer file system When you click “Download” on a file you’ve found online, you probably know where to go to find it: your Downloads folder. This is a folder, or directory (this lesson will use the terms folder and directory interchangeably). You may not know that the Downloads folder, as well as the Desktop, Documents, Pictures, and other built-in folders, are contained within another, higher-level directory: your user directory. Your user directory – on an institutional computer, this is probably named after your institutional ID – is inside a Users folder, which is inside your computer’s root folder. In this paragraph, we have journeyed backwards from Downloads to the root directory. We can represent this route in a single line, which will look slightly different if your computer is a Mac or Windows machine: Mac: /Users/yourname/Downloads Windows: C:\\Users\\yourname\\Downloads For Mac machines, the first forward slash (/) represents the root of the file structure; this is the same as the C: in the Windows example. In either case, this is the path, or address, to the Downloads folder from the root. In addition to saving files in the built-in folders on your computer, you can, of course, make your own folders. For example, you might make a folder called Fall-2026 inside your Documents folder, with the intention of storing all your coursework for the Fall 2026 semester within. Because it is inside the Documents folder, Fall-2026 is a subdirectory of Documents. You could make more subdirectories within Fall-2026, maybe to organize coursework by course. This might look something like this: ![A diagram showing a hierarchical file structure.][photos/organizing-files.png] Just like we wrote a path to the Downloads folder before, we can construct a path to a given coursework file in this imaginary structure. Let’s say we’re interested in the project_data.xlsx folder, which is in the subdirectory for the course si676. We’ll write the path assuming that it’s on a Windows machine belonging to the user kevinq. Which of the following is the correct path? A. Users B. /Users/kevinq/Documents/Fall-2026/si676/project_data.xlsx C. **C: When a path begins all the way at the root directory, like the example above, it’s called the absolute path to the file or folder of interest. Imagine that this is like the full list of directions from an origin – your place of residence – to a destination – your office or favorite study area on campus. The absolute path shows every nested folder that needs to be navigated to reach the target. But what if we’re already in the Fall-2026 folder? In our directions analogy, this would be like needing directions to your destination from somewhere on campus: you are already part of the way there, and you only need directions for the final few steps. We need to construct a relative path, which gives the directions to our target file (or folder) relative to wherever we currently “are” in the hierarchy. From Fall-2026 to project_data.xlsx, on a Windows computer, this would look like this: .\\si676\\project_data.xlsx In relative paths, the period (.) at the beginning represents the current working directory, or where you currently “are” – in this case, the Fall-2026 folder. Now that you have an in-depth understanding of the file system on your computer, let’s cover some recommendations for how to organize your project files within that system. 4.3.2 A recommended approach to project organization Given that what files are used and produced during a research project varies widely across projects, labs, and disciplines, the advice here will be broad, and it is up to you to consider how it can be applied to your work. The main thing to remember is that at this moment, while you are managing your research project, you are the expert in the data and associated files. This is both a responsibility and an opportunity: regardless of the state of the project when you came aboard, you can leave it better than you found it by adapting these recommendations for your files. Practice 1: Keep project materials together As a first step, gather all the “stuff” of your project – raw data files, processed data, documentation, research code, image files, and anything else involved in the work – into one location. This might be a Google Drive or Box folder, or a subfolder you’ve created in your Documents or elsewhere on your computer. (NOTE: This lesson does not give recommendations on backing up files, creating duplicate copies, or storing some aspects of the project on other systems, like pushing your code to GitHub. For more information on storage, please consult the Keeping Your Data Secure lesson.) Aside from knowing where everything is, one benefit of starting with all your materials in a single location is that you can easily share the project file with new collaborators and be assured that they have access to everything they need for onboarding. Additionally, if you work with your materials computationally, regardless of how you further organize the materials within the main project folder, the files will all be fairly near to each other. This lends itself to short and uncomplicated relative paths from one file to another: you can set your working directory to the main project folder and jump around within subdirectories as needed. Practice 2: Group files into logical subdirectories From this main folder, we recommend that you subdivide materials by file type and/or by project workflow. This is where the nature of your specific project will dictate how you apply this recommendation: it might make sense to keep all code in a subfolder called code or scripts, or it might better align with your workflow to store each individual script in the subfolder for its process – one script in a data-cleaning folder and another in a data-visualization folder, for example. It is also worth considering what a “thing” is for your project materials: is there a one-to-one relationship between files and things? For example, does a single image file stand on its own, with one image per research object, or do you have several images of the same object, which need to be treated as a unit? Is the data for a given process a single file, or is your data stored in a complex format that actually requires several different file types? (A common example of this is spatial data. The ESRI shapefile is actually a collection of several geographic and indexing file extensions.) In any case where there is a many-to-one relationship between files and the “thing” that the files comprise, be sure to store all the files for a given research object in their own folder. It’s generally a good idea to keep “raw” data (where raw means whatever state the data is in when you generate/access/receive it) separate from processed/altered versions of that data. Create a new subfolder, in the main project folder or nested within the appropriate subfolder, and have all your data manipulating or processing code output the new data files into that location. It’s crucial that you and any of your current or future collaborators can quickly tell the difference between raw and altered data, so be sure to name the folders where versions of the data are stored in a clear and consistent manner. Practice 3: Employ file naming conventions The goal of establishing a file naming convention is to avoid the common problem of trying to determine whether “project_data(2).xlsx” or “project_data_final(1).xslx” is the version of the data you need. Conventions – rules that dictate what information should be included in a file name, and how that information should be formatted – ensure that versioning information is clear and that you can tell at a glance what any file in your project contains. What conventions make sense for you will depend closely on the file formats, quantity, and complexity of your project materials, and you may have limited autonomy to rename existing project files. The suggestions below can be adapted as needed, and if you are unable to apply them retroactively to the materials you’ve inherited in your lab, you can still apply them to the materials you produce. First, if you use any computer programs in your research, we strongly recommend getting into the habit of not including spaces in your folder and file names. Many applications fail to recognize white spaces and will handle your files/folders in unexpected ways – for example, for a file named “Cover Letter.pdf” a program might see “Cover” and “Letter.pdf” as two separate objects. Instead, it’s common to use an underscore (_) where a white space would normally occur. You can also use dashes (-) or camel casing (writeFileNamesLikeThis). From that starting point, imagine your folders and files as moving boxes, and the names you give them as the labels you scrawl on the boxes (“Pots - Kitchen” or “FRAGILE!”). What information do you need to include so that your future self or collaborators will know where things are? Taking into account how your computer system will sort files, put the most important information first: if you want to be able to search for files by date, start every file with its creation date. Be sure to format dates uniformly; we recommend the format YYYYMMDD. Other components of a file or folder name might include abbreviations or full sample IDs, lab location, researcher name, or other important information. If you use abbreviations, be sure to define them in your README. To keep track of versions of the same data, end the file name with a standardized versioning annotation, like “_v03”. Finally, generally avoid special characters, like %!$@. Some resources to help you plan your file naming conventions are linked below, but we want to highlight our favorite, which is Kristin Briney’s File Naming Convention Worksheet. Whatever conventions you determine are appropriate for your work, document them! This will be useful to remind yourself, and it also gives you the opportunity to pass the information on as onboarding material for future members of the lab. 4.4 Conclusion Organizing your materials with a logical flow and using clear and consistent naming conventions is a massive first step in improving your project’s data management practices, and these recommendations should not take you more than a working session to implement. Once all your files are in working order, your next steps will be to ensure that the data within those files is standardized and documented as much as possible and that the code in your scripts is well-annotated. Along the way, you can begin to fill out READMEs for your project as a whole and for individual files or processes; these will be slotted into the file organization you established in this lesson and provide a “Start Here!” place for future collaborators to become oriented to the project. 4.5 Exercises 4.6 Further Readings UC Davis DataLab lesson on navigating the file system in the command line: https://ucdavisdatalab.github.io/workshop_introduction_to_the_command_line/navigation.html MIT Worksheet: Naming and Organizing your Files and Folders by MIT Libraries Data Management Services.Copyright © 2020-04-24 MASSACHUSETTS INSTITUTE OF TECHNOLOGY, licensed under a Creative Commons Attribution 4.0 International License except where otherwise noted. \\[[https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)\\]. Access at https://www.dropbox.com/scl/fi/1zd63iszw33rh4hjcu1dl/Worksheet_fileOrg.docx?rlkey=q0t25t1wttp4qx2p1ne39qfhd&amp;dl=0. UC Merced LibGuide on File and Folder Organization: https://library.ucmerced.edu/file-and-folder-organization#:~:text=File%20Structures,want%20when%20looking%20up%20files Briney, Kristin A. 2020. “File Naming Convention Worksheet”. June 2. https://doi.org/10.7907/894q-zr22. "],["annotating-code.html", "Chapter 5 Annotating Code 5.1 Lesson Objective 5.2 Key Terms 5.3 Lesson 5.4 Conclusion 5.5 Exercises 5.6 Further Readings", " Chapter 5 Annotating Code Or, how to make your code understandable to your collaborators and future self. 5.1 Lesson Objective Implement low-lift practices to ensure research code is human-readable, reusable, and shareable. 5.2 Key Terms Absolute Path: The path, or address, from the root of a computer’s file system to a directory or file of interest. Dependencies: Here, a software component (like a program or library) that is required for your code to run. For example, R users frequently use the tidyverse, making their code dependent on the tidyverse’s packages. Global Variable: Although the term varies between languages, here we mean a variable that is accessible throughout the script (i.e., not defined locally within a function). Human Readability: How understandable a code file’s purpose, function, and underlying logic are to a human, rather than the computer executing the code. Libraries/Packages: Although the term varies from language to language, these are packaged up units of code that are broad and reusable. Often, they are written by the language’s users. Python packages are available on the Python Package Index (PyPI), R libraries on the Comprehensive R Archive Network (CRAN), and Stata programs on the Boston College Statistical Software Components (SSC). Modularization: The practice of dividing something – in this case, code – into separate, independent modules or processes. Relative Path: The path, or address, to a directory or file of interest, relative to the current working directory. Research Code: Here, the scripts that you and your lab use to work with your research data – for example, to scrape, clean, modify, combine, analyze, or visualize it. Working Directory: The directory within your computer’s file system that you are currently “in;” the directory that a computer program can currently see or access. 5.3 Lesson This lesson discusses simple ways to increase the sustainability of your research code. By sustainability, we mean the likelihood that your code will be understandable and reusable across time or in different hands. While “code” can mean a million things, here we are referring to the scripts that you and your lab are actively using to work with your research data – for example, to scrape, clean, modify, combine, analyze, or visualize it. (This lesson will use the terms “script” and “code” interchangeably, and is not necessarily designed to apply to “software,” where software is referring to a program that your team wants to package and make broadly available as a deliverable in and of itself.) These practices will be applicable to your code whether you work in R, MATLAB, Python, Stata, or another language, although you may need to use your own expertise to adapt them appropriately. Applying them will be a boon to yourself in the future, as you reacquaint yourself with the details of a project you worked on weeks, months, or even years earlier; or to whomever may be the recipient of your project if it is handed off to a new cohort of researchers. It’s helpful to imagine your research code like a book that someone would read from beginning to end: Write the title page. The easiest way you can improve the sustainability of your research code is to start every script with a header: a chunk of comments providing basic metadata about the script and its purpose. A single comment that says who wrote the code, when, and for what purpose is better than nothing, but ideally your header will include more information, like your contact information (an email address you anticipate checking even if you move on from your current institution would be best) and that of any collaborators. \\[In RMarkdown, an example R script header here\\] While it won’t take long to write a header each time you start a new script, you can also create and reuse a template, which has the added benefit of standardizing the information you provide in all your code files. Both RStudio and Visual Studio Code allow users to define snippets, which makes it easy to quickly insert a header template (resources for doing this are linked in the “Further Readings” section below). If the environment where you write your research code does not support this, you could simply write a template in a Google Doc, Word document, or text file and copy/paste it into your scripts. Establish the setting. After the header, begin your script by loading in or installing any packages/libraries you will be using. Even if you don’t call a function from a package until the middle or end of your script, it’s ideal to load everything all at once at the top, and allow future users of the script to quickly see what dependencies it will require. Next, set your working directory using a relative path. This requires that you have organized your code file(s) and any other files you will require the code to interact with (e.g., image files it will process, or a CSV it will read in and manipulate) so that they are statically related to each other. For example, you might store your raw data in a folder called “raw-data,” and your processing code in a folder called “processing-scripts,” with both folders nested within the overall project folder. The script and the data it needs to access are then parallel to each other in the folder hierarchy, and this will remain true no matter where the overall project folder is located – on GitHub, maybe, or in a colleague’s Downloads folder after you have emailed them a zipped version of the project files. NOTE: see the Navigating and Organizing Files lesson for more information on organizing files so that they are statically related and on the difference between relative and absolute file paths. Introduce the characters. You should also declare any global variables (in Stata, global macros) toward the top of the script and include comments that define what the variables store (see below for more information on writing comments). If you are writing your own functions (for Stata, programs) and it makes sense to include them in this script – either because of the conventions of the language that you’re writing in, or because it is a relatively small amount of code – it’s a good idea to include the functions here, toward the top of the script, before calling them below. (The exception to this rule is MATLAB; local functions should be declared at the end of script files in this case.) If you are a Python programmer, you should be familiar with docstrings, a special kind of comment used to define the purpose, parameters, and output of a user-defined function. We highly recommend taking the time to use this documentation standard; it’s a great, low-lift way to ensure that your functions are understandable and reusable in the future. In MATLAB, this is accomplished through help comments, which are placed immediately after the function line. In other languages, you may emulate docstrings for your functions/programs by simply adding comments, although this will not result in you being able to use help commands to access the documentation, as a docstring in Python allows (example below). For this feature, R users may be interested in the docstring package, written by Dason Kurkiewicz and available on CRAN. Example of a docstring in Python, and the output of calling the help() function. Organize the plot and title your chapters. Like novels on a shelf, the “plot” of every project’s code will differ widely, so the advice here is broad: organize the rest of your code following the order of the research workflow, and use comments as “chapter” or section headings. If the process is simple, this may look like a section called “Cleaning” that contains the lines of code used to prepare the raw data for analysis, followed by sections for each of the analyses or models. Ideally, the outputs of the analyses will be saved externally at the end of the script or the end of each section. For much larger projects, we might imagine the project not as a book, but as a series, with entirely separate code files for data cleaning, writing functions, calling those functions to analyze the data, or visualizing results. This scenario is an example of modular programming, where we separate components of a project so that each completes a single task in the overall workflow. Strategically employ narration and description. Like the other practices here, including descriptive comments throughout your code will help ensure that future readers – including your future self – can quickly move from orienting themselves to the code to making use of it. Comments are usually added to code when… An action will result in a new or modified variable, An action will alter a dataset/frame, An action will result in an output (to console or to your machine), User input will be required, Conditions must be met (e.g. adding argument) before running a code block, Users need to alter the code for their needs, A function is defined, Or in any other case when an explanation is needed or the underlying logic should be articulated. The table below shows the syntax for adding comments in each of the languages discussed in this lesson. Language In-Line One-Line Block Python # # “”” comment here “”” R # # Not supported; use multiple 1-line comments Stata // * /* comment here */ MATLAB % % %{ comment here %} 5.4 Conclusion In sum, implementing these practices – to the degree appropriate to your work, and in keeping with the conventions of the language in which you are writing – will make your code as readable as a story book. Additionally, you can apply these practices to code that you inherit from previous lab members. Although it will require you to dig into the code to understand it fully, adding thoughtful annotations will ensure that your successors can more easily and quickly jump into the work. 5.5 Exercises 5.6 Further Readings Resources for setting header templates: Visual Studio Code: https://code.visualstudio.com/docs/editor/userdefinedsnippets R Studio: https://timfarewell.co.uk/my-r-script-header-template/ Stata: https://randrescastaneda.rbind.io/post/dotemplate/ Some content derived from: Cooper, T., Janée, G., Maye, K., Ruhs, N., Erickson, S. 2024. Materials for Code Data Curation Curriculum. Data Curation Network GitHub Repository. Global and Local Variables in Python article: https://www.geeksforgeeks.org/global-local-variables-python/ "],["writing-data-documentation.html", "Chapter 6 Writing Data Documentation 6.1 Lesson Objective 6.2 Key Terms 6.3 Lesson 6.4 Conclusion 6.5 Exercises 6.6 Further Readings", " Chapter 6 Writing Data Documentation Or, how and why to write READMEs and data dictionaries. 6.1 Lesson Objective Learn to write README-style documentation and data dictionaries for an ongoing research project. 6.2 Key Terms Data Dictionary: Documentation that accompanies a dataset, listing and defining each variable and providing information on data types, conventions followed, and valid/possible values for each variable. Data Standard: An agreed-upon, technical specification for how data should be formatted. Metadata: Literally, data about data. In this case, information about a dataset, usually including its creators and funders, structure and dimensions, dates of creation and alteration, collection methodology, and reuse permissions and requirements. README: A document, typically written in Markdown or as plain text, that serves as the front page or abstract of a project’s materials – for example, the source code and data files used in a research project – and conveys information about the purpose, creation, organization, and use of the materials. Variable: Here, a column in a dataset. 6.3 Lesson Imagine your supervisor emails you a zipped folder, telling you that it contains all the raw data files from the two doctoral researchers who worked on this project before you joined the lab. You download and unzip the folder, and inside find two subfolders, one named for each past student, both containing versions of the same project materials. The first student’s folder contains dozens of data files, some of which share almost identical – and identically uninformative – filenames. You open one of the files and realize that the measurements were taken with an instrument that your lab doesn’t own: this must be data shared from some other lab, but there is no information about where it came from or how it was collected. You can tell that orienting yourself to this data will take a long time, and you’re not sure you will have all the answers you need after digging through the folder’s contents. Before resigning yourself to this fate, you check the second student’s folder. You are surprised and delighted to find that the main level of this folder contains a single file, called README.txt, and several subfolders named intuitively after the datasets they contain. Each subfolder includes a data dictionary for its data files. Opening one, you find all the metadata you needed to know about the secondary data – the protocol the researchers followed, the definition of each variable and more. The main-level README lists all the contents of the subfolders, provides dates and descriptions of when and where the datasets were collected and updated, and details the software you will need to visualize the data. Which version of the data will you choose to work with? You can probably recognize immediately that the project materials that are organized with documentation will be the more efficient and more accurate way forward. This is exactly the purpose of READMEs – to introduce project materials to a new audience, or in this case, a new lab member. Relatedly, a data dictionary serves to give new users an immediate sense of the structure of a tabular dataset, without even needing to open the data file itself – which is especially helpful in cases where the data is stored in a proprietary format or would require a good deal of computing to explore. 6.3.1 READMEs A README is a document, typically a plain text file (.txt) or written in Markdown (.md), that provides metadata about a dataset in a standardized format. While the filename doesn’t need to be all capitalized, this is a cultural norm in software development, where the README serves as the front page of a program’s GitHub repository, and the norm seems to have stuck around in research data management, too. Below, we introduce the typical headings of a README and some recommendations for filling them out. General Information: Begin your project-level README with some basic header information: a name for the project; name, affiliation, email address, and ORCID IDs for each collaborator; date range for the project (e.g. funding period); and any other broad information. For dataset-level README documentation, this section should include more specific information about when the data was collected, who collected it, and where (as appropriate). If the document is describing secondary data (i.e., your lab did not collect it/does not own it, but you have access to reuse it), this section should provide information about the source (e.g., government agency or organization), date of access/download, and licensing information (see MIT’s template for secondary data sources for more information). ——————————— TEMPLATE SNIPPET: # TITLE OF PROJECT ## GENERAL INFORMATION COLLABORATOR INFORMATION Name: Role: ORCID: Institution: Email: Name: Role: ORCID: Institution: Email: PROJECT FUNDING PERIOD: FUNDING: ——————————— File Overview &amp; Folder Structure: In the following section, provide an overview of the organization of project materials (or, in the case of dataset-level READMEs, an overview of the file(s) that constitute the dataset). This should include a listing of each file, exactly as it is named, with a brief description of what it contains, and should note the file format (e.g., XML, TIFF, CSV, SHP, WAV, JSON). The information for each file should also include its creation date and any date(s) it was altered or updated. If appropriate, structure this section to reflect or capture the project’s folder structure – list files under the folder in which they are contained. This is also an appropriate section of the README in which to describe any file naming conventions. Since this is a README for active and internal use, you can use this as an opportunity to orient new collaborators to the organization and conventions of the project materials, thus encouraging them to follow and maintain that structure. ——————————— TEMPLATE SNIPPET: ## FILE OVERVIEW Directory Name: File 1: Description: Format: Creation Date: Update Date(s): File 2: Description: Format: Creation Date: Update Date(s): Subdirectory Name: File 1: Description: Format: Creation Date: Update Date(s): … Directory Name: File 1: Description: Format: Creation Date: Update Date(s): … … FILE NAMING CONVENTIONS ——————————— Methodology &amp; Access Information: As appropriate to your project, provide a description of the protocols used to generate/collect the data. This can also include a description of the methods used to clean/process data, as well as a list of software and dependencies needed to view/interpret/process/visualize data. Other relevant information might include requirements for conditions or calibration and quality assurance procedures. As this is an in-use version of a README, you can also use this section to provide more detailed instructions to future collaborators. ——————————— TEMPLATE SNIPPET: ## METHODOLOGY &amp; ACCESS INFORMATION DESCRIPTION OF COLLECTION METHODS: DESCRIPTION OF PROCESSING: INSTRUMENT- OR SOFTWARE-SPECIFIC INFORMATION: INSTRUCTIONS: ——————————— Data Descriptions: For each dataset, provide information on its dimensions (how many rows and columns it contains), its standard for recording missing data, definitions of columns, units of measurements, abbreviations, and specialized formats or data standards. This information constitutes a dataset’s data dictionary, which is explained in detail below. Change Log: While you would not include this information in a final, archival version of a README, it’s not a bad idea to include a scratchpad area at the end of your active documentation. You can use this space to keep a running log of changes you have made to the project folder structure, new outputs, or other edits. This might also include more onboarding information for future collaborators, like an “here’s where I left off” note, or notes to yourself about information that you still need to fully orient yourself to the project materials. ——————————— TEMPLATE SNIPPET: ## CHANGE LOG CHANGES: SCRATCHPAD: ——————————— 6.3.2 Data Dictionaries Whereas you can think of a README as a project-level form of documentation, data dictionaries serve the same purpose at the dataset level for tabular data. Strictly speaking, the dictionary only defines a dataset’s structure, conventions, and standards: how many columns (or variables), the data type of each column (for example, an integer or boolean), acceptable values (e.g., “Yes” but not “yes” or “Y”), and standards (date formatting, geographic abbreviations). This information can easily be conveyed in a table. It may be more useful, though, to think of these dataset-level documents as mini-READMEs that contain data dictionaries (another term for this is a codebook). Before the table describing the contents of the dataset, you can include some of the same information that’s in the project-level README, like dates of creation and modification, people involved, and collection protocols, specific to the dataset you’re describing. Ideally, you will accompany each tabular dataset in your project with a data dictionary. Even more ideal would be to follow a standard template for every data dictionary, so that you know exactly where to look to determine, for example, when each dataset was created. These dictionaries can also be copied into the project-level README so that information is available at both levels. For the README-like metadata at the beginning of each data dictionary, we suggest including the following headers, described above: General Information, Methodology, and Edit Log. Below we detail how to create the dictionary itself, formatted as a table, although it is also common and reasonable to write a dictionary as a plain text document. Dimensions: Above your data dictionary table, include a simple annotation of the number of observations (rows) and variables (columns), formatted as # rows x # columns. For example, for a dataset with 350,000 rows and 16 variables, this section will look like this: Dimensions: 350000 x 16 (Or, you can describe this in more detail, but stick to the same format for every dataset.) Missing Values: Also above the data dictionary table, include an indication of how missing data is captured in this dataset. For example, does the data include “NA”s, “Null”s, empty cells, dummy values, or some other symbol or phrase? Are multiple methods used to indicate missing data? Indicate this here. Variable: The first column in your data dictionary table should list the name of every variable in the dataset, spelled and capitalized exactly how they appear in the dataset. This is crucial for a reader to be able to quickly map the information in the data dictionary to the dataset it’s describing. For the dataset we describe in the “Dimensions” section above, this dictionary should have 16 rows, one for each of the 16 variables. Variable Name: The next column should provide a human-readable version of each variable name, as needed. For example, if the real variable name in the dataset is “wght,” this column could say “weight,” clarifying what the variable name means. You can think of this column as showing how each variable name would be styled in the labels of a graph or figure. Units: The third column in your data dictionary should simply record the unit of measurement for each variable. For example, if the variable contains weight, this column will clarify if the weight is measured in grams, pounds, or something else (for plenty of columns, like text columns, this will be N/A). Data Type: This column should record what kind of data is contained in each column. Some possible options might include numeric or integer, character or text, float or decimal, boolean (0/1, Y/N), or date data types. NOTE: This assumes all the data in a given column is formatted uniformly. Although this is ideal, in real research, this is often not the case. For example, participants in a survey might give their age as “26” or “twenty-six,” resulting in a “age” variable that looks like both an integer and a text data type. If this is data you inherited, you may or may not have the opportunity or ability to standardize the data; for now, use the Data Type column in your data dictionary to record all the data types present in a given column of the data. Values: The next column should note the acceptable values, or range of values, for each variable. For numeric or date data, this should include the minimum and maximum values included in the column, so you might need to perform some quick exploratory data analysis on the data to get this information (this might be as simple as sorting the data in a spreadsheet software – but be sure to sort the entire spreadsheet, not only one column!). For boolean values, you might use this column to be explicit about what the values mean, if you know it: is 0 a stand-in for “no,” “absent,” or something else? If the column is a text data type and contains only a finite number of possible values, they should be listed or described here. Examples of this might include a “state” column, where the values could be noted here as “2-letter abbreviations of the 50 U.S. states,” or a “life stage” column that only contains the values listed here: “Juvenile, Immature, Adult”. Standard: This column lists any data standards used in the described variable. A data standard is an agreed-upon specification for how data should be formatted – by agreed-upon, we mean that some organizing body, like the Library of Congress, has established the standard, or that it is a commonly accepted convention within your discipline. In many cases this looks like a controlled vocabulary of acceptable values. For example, the Getty Research Institute Vocabularies provide standardized stylings to record or describe geographic names and cultural objects. A standard could also specify how data should be formatted, like the International Organization for Standardization (ISO) 8601 date and time standard, which specifies that date should be formatted uniformly as YYYY-MM-DD. As in the value of data types, it is outside of your control whether data you inherited was formatted following data standards, and it may very well be outside of your control whether it can be transformed to align with the conventions of your discipline – although, it is worth having a conversation about with your supervisor! Regardless, if you know that a variable in the dataset you’re describing follows a data standard, you can record that information here. Description: The final column of your data dictionary can provide a simple, plain-language description of the described variable. This might look something like “weight in grams” or “participant’s date of birth”. 6.4 Conclusion READMEs and data dictionaries are typically associated with the end of a project, acting as “what’s inside” guides to accompany data that is ready to be archived or deposited in a repository. So, why bother to do this while you’re still working on the project? One major benefit to drafting a README early in the project is that a single, short document that keeps track of materials helps you stay organized in the complex and evolving ecosystem of in-progress research. Having one source of truth means that you only need to open one file to find materials you need or quickly answer collaborators’ questions about what a variable name means, when a data file was created, or what a folder contains. Along those same lines, maintaining documentation throughout the project timeline means that nothing is left up to institutional memory, whereas waiting until the end of the project risks omitted details. Another major reason to start data documentation now is that you may not be here when the project concludes and the data is ready to be archived or shared. It is very common for graduate students to join a project after it has started and graduate before it has ended. If this is the case for you, you will not have the opportunity to ensure that the data is archived with accurate and up-to-date documentation, but you do have the opportunity, right now, to ensure that when your work is shared with new collaborators, they have all the information they need. Writing a README now is your chance to write the TL;DR1 of your efforts and contributions to the work, and well-formatted data documentation helps ensure that these best practices continue when you have moved on. 1 Too Long; Didn’t Read. 6.5 Exercises Provide an empty spreadsheet for a data dictionary and have them fill it out for 1 of the tabular datasets 6.6 Further Readings https://data.research.cornell.edu/data-management/sharing/readme/ https://georgiasouthern.libguides.com/c.php?g=833713&amp;p=5953142 https://www.markdownguide.org/cheat-sheet/ https://datamanagement.hms.harvard.edu/collect-analyze/documentation-metadata/readme-files MIT template for README for secondary data: https://www.dropbox.com/scl/fi/ij4u7v26s01932pfcd7hr/Template_SECONDARY_DATASET_Readme.txt?rlkey=cmz5u7e2evdb4nytlsmrrllaq&amp;e=1&amp;dl=0 Harvard RDM LibGuide on data dictionaries: https://datamanagement.hms.harvard.edu/collect-analyze/documentation-metadata/data-dictionary Briney workbook Create a Data Dictionary exercise: https://caltechlibrary.github.io/RDMworkbook/documentation.html#data-dictionary ICPSR’s “What is a Codebook?”: https://www.icpsr.umich.edu/web/ICPSR/cms/1983 OSF page How to Make a Data Dictionary: https://help.osf.io/article/217-how-to-make-a-data-dictionary Data.gov page on data standards: https://resources.data.gov/standards/concepts/ "],["keeping-data-safe.html", "Chapter 7 Keeping Data Safe 7.1 Lesson Objective 7.2 Key Terms 7.3 Lesson 7.4 Exercises 7.5 Further Readings", " Chapter 7 Keeping Data Safe Or, how to ensure your data isn’t altered or removed accidentally or intentionally. 7.1 Lesson Objective 7.2 Key Terms 7.3 Lesson We have all been there before: you go to review your article, and your most recent save didn’t take– or worst case, the document is completely gone. Or, you saved your important presentation to a flash drive– and now you’ve lost that flash drive. Now imagine this is your research data, and you can’t access the recent and time-bound data you painstakingly captured. Not only is this frustrating and a setback to the project, it can also feel embarrassing, especially in a team setting. To avoid this, practice robust data security and preservation standards. If you’re entering a project, there are (or should be) structures in place to ensure that data are backed up regularly, version controls to prevent accidental editing, and processes to document your data collection workflows. If there are not, this is a good chance to start the conversation! To the extent you have control over the project data– or at least your data within the project – there are a few rules of thumb you can adopt to help prevent data loss. 7.3.1 Practice 1: Remember 3-2-1 This best practice suggests that, to avoid data loss or corruption, you should save 3 copies of important files in 2 locations with 1 copy stored remotely (e.g., a cloud storage system, such as Google Drive). This can be challenging, especially when working on a team that is actively updating data files. But don’t be daunted! A relatively straightforward way to implement this is to ensure that you have periodic (e.g., weekly, monthly) backups in place. These can be implemented in an automatic fashion so you don’t have to worry about it. If you are concerned, you can connect with your institution’s IT – they can provide more details on your current storage mechanism or provide advice on how to streamline this process. Regardless of the means you use to implement backups, it is important these are documented and understood so you always know where the authoritative version of the working files is. 7.3.2 Practice 2: Only edit raw data when necessary When you are capturing data, you are creating ‘raw’ data. By definition, then, you’ll be editing the raw data. But, after the data collection phase, consider carefully when, if at all, you need to be using the raw and original data. Sometimes data cannot be recreated– take, for example, a weather condition on a certain day a week ago. You will never be able to get that data back– so if you accidentally delete something, you’ve just irrevocably altered your data (gasp). So, when you are conducting analyses, running scripts, using R to mutate, using excel formulas, etc, make a derivative (copy) of the data for analysis. 7.3.3 Practice 3: Save data from key steps As your project progresses, and you create derivatives to run analyses, be sure to keep data files from key steps of your research without keeping every file. You need to keep the key steps – but don’t let digital clutter drown out your work 7.3.4 Practice 4: Keep your guest list up to date As people transition on and off the project, ensure that access is added– and removed. This is especially important if your data have any sort of sensitivity, but is good practice regardless. When someone leaves– even if you think their account will be deactivated by your central IT– you should still plan to remove their account’s access. (Odds are their account won’t be activated again, but sometimes these old accounts can provide backdoors for bad actors) 7.3.5 Practice 5: Ensure continuity in ownership Lastly, consider the ownership of your materials early into your project (but again, better late than never!). If you are working on a team, be thoughtful about who OWNS your data For example, if something is stored in your google drive, your account may be deleted. Ways around this (creating in a shared drive, changing ownership) Some institutions have policies in place that will prevent them from being able to recover your data, even with your approval! So start with the end in mind and make sure it is stored on a team or group owned account 7.4 Exercises 7.5 Further Readings "],["advocating-for-credit.html", "Chapter 8 Advocating for CRediT 8.1 Learning Objective 8.2 Key Terms 8.3 Lesson 8.4 Exercise 8.5 Further Readings", " Chapter 8 Advocating for CRediT Or, how to ensure your labor is recognized and honored. 8.1 Learning Objective Ensure your contributions are documented, advocate for yourself and others, and set boundaries 8.2 Key Terms CRediT: A taxonomy for describing the different types of contributions that typically occur during a research project, from project conception through to publication. ORCID iD: A unique persistent identifier to disambiguate researchers and track research outputs. 8.3 Lesson The reality of being in a term-limited position as a graduate student means that you may not be affiliated with the institution or lab when the project you’re on concludes. However, that does not mean that your work was not critical for completion of the project. To that end, it is important that your contributions are appropriately recognized. This will quite obviously be tricky if you are not there to speak up– but there are practices you can start early that will not only benefit future you, but also future researchers. You can help change culture, in your lab, at your institution, and in your discipline, by ensuring your work (and the work of those that preceded you!) is documented and citable. 8.3.1 Practice 1: Make an ORCID iD One of the best – and easiest! – ways you can start receiving recognition of your work is to make an ORCID iD. These are free, persistent, and unique identifiers for researchers. In the same way that a digital object identifier (DOI) always references a single output (like an article), your ORCID will always be linked to you as you progress through your career. Once you have an ORCID, you can begin entering this information when prompted, especially through publication platforms. Journals, funding agencies, and other repositories have automated connections with ORCID that will allow your name to be connected to your outputs. As you progress through your career, you can also manually update your ORCID profile page to reflect your professional achievements. This makes it easy for others to see, in one spot, all of you’ve accomplished. This is slightly different from your Google Scholar profile, as your ORCID page can capture not just your publications, but also your professional positions, committee work, and even grants received! 8.3.2 Practice 2: Familiarize yourself with CRediT Another low-entry means to start advocating for recognition of your work is to review the Contributor Role Taxonomy (CRediT). This is a widely-adopted set of 14 different types of contributions that typically occur during a research project, from project conception through to publication. ACTION: Review the different CRediT roles, first by yourself and then with your PI, to clarify which role(s) you will be associated with. Remember: You can have several roles. Each role can have several assignees. Document the results of this discussion in project documentation (e.g., the README file, link) 8.3.3 Practice 3: Conceptualize the dataset as a standalone product Often, datasets are considered the means to the end, the mechanism by which the publication is produced. This understanding is (slowly) changing, as data is becoming understood as a separate and equally important output. (Idk what else to put here– but something about how data can be used and reused separate from the original intention.) (Needs to be adapted, but: “Poet Henry David Thoreau’s field notes were used to document that migratory bird arrivals and trees and shrubs budding is occurring earlier in the spring, further evidence of global warming (Primack and Gallinat 2016).”) 8.3.4 Practice 4: Advocate for those that preceded you If you are the graduate student who is present as the project concludes, review the named contributors. If you know of anyone who worked on the project, in any of the roles articulated in CRediT, advocate for recognition of their labor. This is a key way to help shape and change behavior that will have an impact beyond your time with the project or lab. Not sure if important to add here, but getting at: Pay it forward. Academia is small– actions of kindness? idk 8.3.5 Practice 5: Set boundaries on your involvement As you near the end of your studies, if it is clear that the project will conclude after your departure, consider carefully how you will be involved after graduation. The practice of remaining involved with the project will differ not only by discipline, but also lead investigator and even the project itself. Consider: Will you continue to work on the project to see the dataset and/or publication completed? Will you present this work? If so, is this something that will be done as part of your new role, or will this require you to volunteer your time? If this is something you will need to do outside of your daily work, consider reviewing these flowcharts (one created for an academic, one more general) to decide if this is actually something you want to take on: Peltzman, S. (2016). Should You Take on a Project?: A Flowchart. Zenodo. https://doi.org/10.5281/zenodo.4694788 Note: this one ends with CONSIDER taking on the project. Does not mean DO take on the project! https://unhustle.com/a-fck-no-flowchart-say-no-without-guilt/ Whatever your decision, be sure this is understood by all parties. And remember: even if you do not continue with the project, you still contributed, and you still deserve recognition. 8.4 Exercise Elevator pitch Write out your talking points. Use CReDiT – what role(s) did you fill? Are there others who preceded you that should have their contributions captured? Are you referring to the data as a standalone product? 8.5 Further Readings A student collaborators’ bill of rights: https://humtech.ucla.edu/news/a-student-collaborators-bill-of-rights/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
